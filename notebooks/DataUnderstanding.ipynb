{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from scipy import optimize\n",
    "from scipy import integrate\n",
    "from scipy import signal\n",
    "\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression(fit_intercept=True)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source : John Hopkins Dataset\n",
    "# Data understanding and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of rows stored: 63042\n",
      " Latest date is: 2020-09-14 00:00:00\n"
     ]
    }
   ],
   "source": [
    "#Get latest data from JH and store in csv\n",
    "def store_relational_JH_data():\n",
    "    ''' Transformes the COVID data in a relational data set\n",
    "\n",
    "    '''\n",
    "\n",
    "    data_path='../data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "    pd_raw=pd.read_csv(data_path)\n",
    "\n",
    "    pd_data_base=pd_raw.rename(columns={'Country/Region':'country',\n",
    "                      'Province/State':'state'})\n",
    "\n",
    "    pd_data_base['state']=pd_data_base['state'].fillna('no')\n",
    "\n",
    "    pd_data_base=pd_data_base.drop(['Lat','Long'],axis=1)\n",
    "\n",
    "\n",
    "    pd_relational_model=pd_data_base.set_index(['state','country']) \\\n",
    "                                .T                              \\\n",
    "                                .stack(level=[0,1])             \\\n",
    "                                .reset_index()                  \\\n",
    "                                .rename(columns={'level_0':'date',\n",
    "                                                   0:'confirmed'},\n",
    "                                                  )\n",
    "\n",
    "    pd_relational_model['date']=pd_relational_model.date.astype('datetime64[ns]')\n",
    "\n",
    "    pd_relational_model.to_csv('../data/processed/COVID_relational_confirmed.csv',sep=';',index=False)\n",
    "    print(' Number of rows stored: '+str(pd_relational_model.shape[0]))\n",
    "    print(' Latest date is: '+str(max(pd_relational_model.date)))\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    store_relational_JH_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data from the URL mentioned above\n",
    "def getLatestDataFromURL(info_type):\n",
    "    if info_type == \"confirmed\":\n",
    "        response = requests.get(\"https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\")\n",
    "    elif info_type == \"deaths\":\n",
    "        response = requests.get(\"https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv\")\n",
    "    elif info_type == \"recovered\":\n",
    "        response = requests.get(\"https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv\")\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    html_table=soup.find('table')\n",
    "    all_rows=html_table.find_all('tr')\n",
    "    jh_data_list=[]\n",
    "    for pos,rows in enumerate(all_rows):\n",
    "        if pos==0:\n",
    "            header_list = [each_col.get_text(strip=True) for each_col in rows.find_all('th')]\n",
    "        else:\n",
    "            col_list=[each_col.get_text(strip=True) for each_col in rows.find_all('td')] #td for data element\n",
    "            jh_data_list.append(col_list)\n",
    "    return jh_data_list,header_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for visualization and modelling\n",
    "def prepareData(jh_data_list,header_list):\n",
    "    header_list.insert(0,'index')\n",
    "    jh_data_df=pd.DataFrame(jh_data_list)\n",
    "    jh_data_df.columns=header_list\n",
    "    #jh_data_df.head()\n",
    "    time_idx=jh_data_df.columns[5:]\n",
    "    country_list=jh_data_df['Country/Region']\n",
    "    jh_data_transformed_df = pd.DataFrame({'date':time_idx})\n",
    "    for each in country_list:\n",
    "        jh_data_transformed_df[each] = np.array(jh_data_df[jh_data_df['Country/Region']==each].iloc[:,5::].astype(int).sum(axis=0))\n",
    "    #jh_data_transformed_df.tail()\n",
    "    time_idx=[datetime.strptime( each,\"%m/%d/%y\") for each in jh_data_transformed_df.date] # convert to datetime\n",
    "    time_str=[each.strftime('%Y-%m-%d') for each in time_idx] # convert back to date ISO norm (str)\n",
    "    jh_data_transformed_df['date']=time_idx\n",
    "    return jh_data_transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_list,header_list = getLatestDataFromURL(\"confirmed\")\n",
    "confirmed_df = prepareData(confirmed_list,header_list)\n",
    "confirmed_df.to_csv('../data/processed/COVID_small_flat_table_confirmed.csv',sep=';',index=False)\n",
    "deaths_list,header_list = getLatestDataFromURL(\"deaths\")\n",
    "deaths_df = prepareData(deaths_list,header_list)\n",
    "deaths_df.to_csv('../data/processed/COVID_small_flat_table_deaths.csv',sep=';',index=False)\n",
    "recovered_list,header_list = getLatestDataFromURL(\"recovered\")\n",
    "recovered_df = prepareData(recovered_list,header_list)\n",
    "recovered_df.to_csv('../data/processed/COVID_small_flat_table_recovered.csv',sep=';',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPopulationDataFromWM():\n",
    "    response = requests.get(\"https://www.worldometers.info/world-population/population-by-country/\")\n",
    "    response.content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    html_table=soup.find('table')\n",
    "    all_rows=html_table.find_all('tr')\n",
    "    jh_data_list=[]\n",
    "    for pos,rows in enumerate(all_rows):\n",
    "        if pos==0:\n",
    "            header_list = [each_col.get_text(strip=True) for each_col in rows.find_all('th')]\n",
    "        else:\n",
    "            col_list=[each_col.get_text(strip=True) for each_col in rows.find_all('td')]\n",
    "            jh_data_list.append(col_list)\n",
    "    jh_data_df = pd.DataFrame(jh_data_list)\n",
    "    return jh_data_df.iloc[:, 1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_data = getPopulationDataFromWM()\n",
    "countries = pop_data[1]\n",
    "population = pop_data[2]\n",
    "pop_df = pd.DataFrame(columns=countries)\n",
    "pop_df.loc[len(pop_df)] = population.tolist()\n",
    "pop_df.rename(columns={'United States': 'US'}, inplace=True)\n",
    "pop_df.to_csv('../data/processed/population.csv',sep=';',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doubling rate and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the test slope is: [2.]\n",
      "            date state country  confirmed  confirmed_filtered  confirmed_DR  \\\n",
      "59956 2020-09-10    no      US  6396100.0           6402419.2    184.137066   \n",
      "59957 2020-09-11    no      US  6443652.0           6440932.0    153.403356   \n",
      "59958 2020-09-12    no      US  6485123.0           6479620.0    144.718219   \n",
      "59959 2020-09-13    no      US  6519573.0           6518722.5    170.777062   \n",
      "59960 2020-09-14    no      US  6553652.0           6557825.0    190.268334   \n",
      "\n",
      "       confirmed_filtered_DR  \n",
      "59956             180.980210  \n",
      "59957             169.810423  \n",
      "59958             166.863307  \n",
      "59959             166.595103  \n",
      "59960             166.708586  \n"
     ]
    }
   ],
   "source": [
    "def get_doubling_time_via_regression(in_array):\n",
    "    ''' Use a linear regression to approximate the doubling rate\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        in_array : pandas.series\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        Doubling rate: double\n",
    "    '''\n",
    "\n",
    "    y = np.array(in_array)\n",
    "    X = np.arange(-1,2).reshape(-1, 1)\n",
    "\n",
    "    assert len(in_array)==3\n",
    "    reg.fit(X,y)\n",
    "    intercept=reg.intercept_\n",
    "    slope=reg.coef_\n",
    "\n",
    "    return intercept/slope\n",
    "\n",
    "\n",
    "def savgol_filter(df_input,column='confirmed',window=5):\n",
    "    ''' Savgol Filter which can be used in groupby apply function (data structure kept)\n",
    "\n",
    "        parameters:\n",
    "        ----------\n",
    "        df_input : pandas.series\n",
    "        column : str\n",
    "        window : int\n",
    "            used data points to calculate the filter result\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_result: pd.DataFrame\n",
    "            the index of the df_input has to be preserved in result\n",
    "    '''\n",
    "\n",
    "    degree=1\n",
    "    df_result=df_input\n",
    "\n",
    "    filter_in=df_input[column].fillna(0) # attention with the neutral element here\n",
    "\n",
    "    result=signal.savgol_filter(np.array(filter_in),\n",
    "                           window, # window size used for filtering\n",
    "                           1)\n",
    "    df_result[str(column+'_filtered')]=result\n",
    "    return df_result\n",
    "\n",
    "def rolling_reg(df_input,col='confirmed'):\n",
    "    ''' Rolling Regression to approximate the doubling time'\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        col: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        result: pd.DataFrame\n",
    "    '''\n",
    "    days_back=3\n",
    "    result=df_input[col].rolling(\n",
    "                window=days_back,\n",
    "                min_periods=days_back).apply(get_doubling_time_via_regression,raw=False)\n",
    "\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_filtered_data(df_input,filter_on='confirmed'):\n",
    "    '''  Calculate savgol filter and return merged data frame\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        filter_on: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_output: pd.DataFrame\n",
    "            the result will be joined as a new column on the input data frame\n",
    "    '''\n",
    "\n",
    "    must_contain=set(['state','country',filter_on])\n",
    "    assert must_contain.issubset(set(df_input.columns)), ' Erro in calc_filtered_data not all columns in data frame'\n",
    "\n",
    "    df_output=df_input.copy() # we need a copy here otherwise the filter_on column will be overwritten\n",
    "\n",
    "    pd_filtered_result=df_output[['state','country',filter_on]].groupby(['state','country']).apply(savgol_filter)#.reset_index()\n",
    "\n",
    "    #print('--+++ after group by apply')\n",
    "    #print(pd_filtered_result[pd_filtered_result['country']=='Germany'].tail())\n",
    "\n",
    "    #df_output=pd.merge(df_output,pd_filtered_result[['index',str(filter_on+'_filtered')]],on=['index'],how='left')\n",
    "    df_output=pd.merge(df_output,pd_filtered_result[[str(filter_on+'_filtered')]],left_index=True,right_index=True,how='left')\n",
    "    #print(df_output[df_output['country']=='Germany'].tail())\n",
    "    return df_output.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_doubling_rate(df_input,filter_on='confirmed'):\n",
    "    ''' Calculate approximated doubling rate and return merged data frame\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        filter_on: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_output: pd.DataFrame\n",
    "            the result will be joined as a new column on the input data frame\n",
    "    '''\n",
    "\n",
    "    must_contain=set(['state','country',filter_on])\n",
    "    assert must_contain.issubset(set(df_input.columns)), ' Erro in calc_filtered_data not all columns in data frame'\n",
    "\n",
    "\n",
    "    pd_DR_result= df_input.groupby(['state','country']).apply(rolling_reg,filter_on).reset_index()\n",
    "\n",
    "    pd_DR_result=pd_DR_result.rename(columns={filter_on:filter_on+'_DR',\n",
    "                             'level_2':'index'})\n",
    "\n",
    "    #we do the merge on the index of our big table and on the index column after groupby\n",
    "    df_output=pd.merge(df_input,pd_DR_result[['index',str(filter_on+'_DR')]],left_index=True,right_on=['index'],how='left')\n",
    "    df_output=df_output.drop(columns=['index'])\n",
    "\n",
    "\n",
    "    return df_output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_data_reg=np.array([2,4,6])\n",
    "    result=get_doubling_time_via_regression(test_data_reg)\n",
    "    print('the test slope is: '+str(result))\n",
    "\n",
    "    pd_JH_data=pd.read_csv('../data/processed/COVID_relational_confirmed.csv',sep=';',parse_dates=[0])\n",
    "    pd_JH_data=pd_JH_data.sort_values('date',ascending=True).copy()\n",
    "\n",
    "    #test_structure=pd_JH_data[((pd_JH_data['country']=='US')|\n",
    "    #                  (pd_JH_data['country']=='Germany'))]\n",
    "\n",
    "    pd_result_larg=calc_filtered_data(pd_JH_data)\n",
    "    pd_result_larg=calc_doubling_rate(pd_result_larg)\n",
    "    pd_result_larg=calc_doubling_rate(pd_result_larg,'confirmed_filtered')\n",
    "\n",
    "\n",
    "    mask=pd_result_larg['confirmed']>100\n",
    "    pd_result_larg['confirmed_filtered_DR']=pd_result_larg['confirmed_filtered_DR'].where(mask, other=np.NaN)\n",
    "    pd_result_larg.to_csv('../data/processed/COVID_final_set.csv',sep=';',index=False)\n",
    "    print(pd_result_larg[pd_result_larg['country']=='US'].tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
